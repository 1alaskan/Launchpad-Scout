---
title: "Job posting volume over time"
format: html
---

# Step three of first project requriemnt (ELT Pipeline)
3. Job posting volume over time 
        - JobSpy (open source Python, scrapes Indeed/LinkedIn)

# Goal
 Measure how actively each startup is hiring right now, which is the first time varying signal that distinguishes growing companies from stagnant ones, and add that data to the data set

TLDR
1. Installed JobSpy (worked around Python 3.13 compatibility issues)
2. Tested Indeed
    -too generic returned wrong companies
3. Switched to LinkedIn
    -accurate matches
4. Built exact-match filter to drop false positives (generic company names like "Anything" pulling unrelated results)
5. Tested on 5 companies successfully
    -got 8 clean matches
6. Ran full scrapt e871 companies (~45 min scrape)

```{python}
import pandas as pd

df = pd.read_parquet(r"C:\Users\spink\OneDrive\Desktop\Personal Projects\Startup Momentum Prediction Pipeline\Data\companies_clean.parquet")

print(f"Shape: {df.shape}")
print(f"\nColumns:")
for col in df.columns:
    print(f"  {col}")
```

```{python}
pip install numpy
```

```{python}
pip install python-jobspy --no-deps
```

```{python}
pip install requests beautifulsoup4 tls_client regex
```

```{python}
pip install markdownify pydantic
```


```{python}
jobs = scrape_jobs(
    site_name=["linkedin"],
    search_term="Artie",
    location="San Francisco, CA",
    results_wanted=5,
)
print(jobs.shape)
if len(jobs) > 0:
    print(jobs[["title", "company", "location"]].to_string())
```

```{python}
from jobspy import scrape_jobs
import pandas as pd
import time

companies = pd.read_parquet(r"C:\Users\spink\OneDrive\Desktop\Personal Projects\Startup Momentum Prediction Pipeline\Data\companies_clean.parquet")
```

```{python}
all_jobs = []
errors = []

for i, row in companies.iterrows():
    name = row["name"]
    city = row["city"]
    state = row["state"]
    location = f"{city}, {state}" if pd.notna(city) and pd.notna(state) else "United States"
    
    try:
        jobs = scrape_jobs(
            site_name=["linkedin"],
            search_term=name,
            location=location,
            results_wanted=25,
        )
        jobs["company_searched"] = name
        all_jobs.append(jobs)
        print(f"[{i+1}/{len(companies)}] {name}: {len(jobs)} postings")
    except Exception as e:
        errors.append({"name": name, "error": str(e)})
        print(f"[{i+1}/{len(companies)}] {name}: ERROR - {e}")
    
    time.sleep(3)


```

Combine and filter to exact matches
```{python}
jobs_df = pd.concat(all_jobs, ignore_index=True) if all_jobs else pd.DataFrame()
jobs_df["match"] = jobs_df.apply(
    lambda r: str(r["company"]).strip().lower() == r["company_searched"].strip().lower(),
    axis=1
)
matched = jobs_df[jobs_df["match"]].drop(columns=["match"])
```

```{python}
jobs_df = pd.concat(all_jobs, ignore_index=True) if all_jobs else pd.DataFrame()
print(f"\nTotal postings: {len(jobs_df)}")
print(f"Errors: {len(errors)}")
```

Save results
```{python}
print(f"Total scraped: {len(jobs_df)}")
print(f"Exact matches: {len(matched)}")
matched.to_parquet(r"C:\Users\spink\OneDrive\Desktop\Personal Projects\Startup Momentum Prediction Pipeline\Data\job_postings_raw.parquet", index=False)
print("Saved to job_postings_raw.parquet")
```


```{python}
print(f"Companies with postings: {matched['company_searched'].nunique()}")
print(f"\nTop 10 by posting count:")
print(matched.groupby('company_searched').size().sort_values(ascending=False).head(10))
```